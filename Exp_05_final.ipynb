{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**:  Tina Yung-Fang Tu\n",
    "**Time Spent**: 10hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please set the path to your data directory here\n",
    "path = \"./EPI511/\"\n",
    "\n",
    "# please use the following function (or something like it) to read files\n",
    "def pname(name):\n",
    "    '''Prepend the path to the filename'''\n",
    "    return path + '/' + name\n",
    "\n",
    "def popen(name):\n",
    "    '''Open file in the path'''\n",
    "    return open(pname(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### functions to read in data ##################\n",
    "def read_snp(file):\n",
    "    '''Read a snp file into a pandas dataframe'''\n",
    "    return(pd.read_table(\n",
    "        file,\n",
    "        sep='\\s+', # columns are separated by whitespace\n",
    "        # names of the columns\n",
    "        names=[None, 'chromosome', 'morgans', 'position', 'ref', 'alt'],\n",
    "        index_col=0))\n",
    "\n",
    "SNPs = read_snp(path + 'HapMap3.snp') \n",
    "\n",
    "def get_chr_range(chromosome):\n",
    "    '''Returns the range of positions where SNPs for a chromosome are kept'''\n",
    "    filt = SNPs.query('chromosome=={}'.format(chromosome))\n",
    "    start = SNPs.index.get_loc(filt.iloc[0].name)\n",
    "    stop  = SNPs.index.get_loc(filt.iloc[-1].name) + 1\n",
    "    return(start, stop)\n",
    "\n",
    "def read_geno(file):\n",
    "    '''Reads a geno file into a masked numpy matrix'''\n",
    "    return(np.genfromtxt(\n",
    "        file,               # the file\n",
    "        dtype='uint8',      # read the data in as 1-byte integers\n",
    "        delimiter=1,        # 1-byte width data\n",
    "        missing_values=9,   # 9 indicates missing data\n",
    "        usemask=True        # return a masked array\n",
    "    ))\n",
    "\n",
    "def read_geno_pop_chr(pop, chromosome):\n",
    "    '''Reads a slice of a geno file into a masked numpy matrix'''\n",
    "    f = open(path + pop + '.geno')      # open the file\n",
    "    (start, stop) = get_chr_range(chromosome)\n",
    "    s = it.islice(f, start, stop) # slice the file only keeping SNPs of chr\n",
    "    return read_geno(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Polygenic prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a set of 1,000 unlinked SNPs in the 112 CEU individuals defined by the first SNP and every 125th SNP thereafter (SNPs 0, 50, …, 124875, if the first SNP is SNP 0).  Assign the first 100 SNPs as causal SNPs and other 900 SNPs as non-causal SNPs. Simulate quantitative phenotypes for CEU individuals by assuming that causal SNPs have effect size per normalized genotype = 0.1 (note that this is different from effect size per allele) and null SNPs have eff75ect size per normalized genotype = 0.  Label this CEU data as “training data”.  Use the same effect sizes to simulate “test data” consisting of real genotypes and simulated phenotypes in 88 TSI individuals.  Implement a polygenic prediction scheme using all 1,000 SNPs in which you use training data to estimate ATT effect sizes (i.e. correlations between genotypes and phenotypes) and then use estimated effect sizes to build predicted phenotypes in test data.    (TSI test data genotypes can be normalized using allele frequencies from CEU training data.) (Note that because this is a simulation, the true effect sizes are known.  However, the true effect sizes should not be used to build predicted phenotypes).  What is the prediction r2 of the predicted phenotypes (vs. true phenotypes) in the test data?  Is the prediction r2 significantly different from 0?  Does the prediction r2 agree with the derivation provided in Week 9 slides?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads a slice of a geno file into a numpy matrix\n",
    "def read_geno_pop_islice(pop, *args):\n",
    "    f = popen(pop + '.geno') \n",
    "    s = it.islice(f, *args)  \n",
    "    return read_geno(s)\n",
    "\n",
    "# Calculate allele frequency\n",
    "def calculate_af(geno):\n",
    "    return np.ma.mean(geno, axis=1).filled(-1) / 2\n",
    "\n",
    "# Normalize geno matrix\n",
    "def normalize_geno(geno, p=None):\n",
    "    if p is None: p = calculate_af(geno)\n",
    "    return ( (geno - (2 * p)[:, np.newaxis]) / np.sqrt(2 * p * (1 - p))[:, np.newaxis]).filled(0)\n",
    "\n",
    "def compute_r2(snp1, snp2):\n",
    "    # Use masks to filter valid data\n",
    "    valid = ~snp1.mask & ~snp2.mask\n",
    "    if np.sum(valid) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    gA = snp1[valid].astype(float)\n",
    "    gB = snp2[valid].astype(float)\n",
    "    \n",
    "    if np.std(gA) == 0 or np.std(gB) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Genotype count for allele\n",
    "    gA_mean = np.mean(gA) \n",
    "    gB_mean = np.mean(gB) \n",
    "\n",
    "    Na = len(gA)\n",
    "    Nb = len(gB)\n",
    "\n",
    "    var_a = ((gA - gA_mean)**2).sum()/Na\n",
    "    var_b = ((gB - gB_mean)**2).sum()/Nb\n",
    "\n",
    "    # Compute r^2\n",
    "    numerator = ((gA *gB).mean() - (gA_mean * gB_mean))**2\n",
    "    denominator = var_a * var_b\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    return numerator / denominator\n",
    "\n",
    "def compute_pearsonr(x, y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "\n",
    "    x_centered = x - x_mean\n",
    "    y_centered = y - y_mean\n",
    "\n",
    "    numerator = np.sum(x_centered * y_centered)\n",
    "    denominator = np.sqrt(np.sum(x_centered ** 2)) * np.sqrt(np.sum(y_centered ** 2))\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0  \n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction correlation (R): 0.34936079770037815\n",
      "Prediction R²: 0.12205296696984454\n",
      "Chi-square: 10.740661093346318\n",
      "p-value: 0.0010480688631516852\n"
     ]
    }
   ],
   "source": [
    "CEU_geno = read_geno_pop_islice('CEU', 0, 125000, 125)\n",
    "CEU_af = calculate_af(CEU_geno)\n",
    "X_CEU = normalize_geno(CEU_geno, CEU_af)\n",
    "\n",
    "# Define effect sizes and phenotype\n",
    "beta = np.zeros(1000)\n",
    "beta[:100] = 0.1\n",
    "Y_CEU = X_CEU.T.dot(beta)\n",
    "\n",
    "# Estimate effect sizes using correlation\n",
    "r_values = np.zeros(X_CEU.shape[0])  # 1000 SNPs\n",
    "for i in range(X_CEU.shape[0]):\n",
    "    r_values[i] = compute_pearsonr(X_CEU[i], Y_CEU)\n",
    "\n",
    "# Load and normalize TSI genotypes using CEU allele frequencies\n",
    "TSI_geno = read_geno_pop_islice('TSI', 0, 125000, 125)\n",
    "X_TSI = np.zeros(TSI_geno.shape, dtype=float)\n",
    "\n",
    "for i in range(1000):\n",
    "    X_TSI[i] = np.ma.filled((TSI_geno[i] - 2 * CEU_af[i]) / np.sqrt(2 * CEU_af[i] * (1 - CEU_af[i])), fill_value=0)\n",
    "\n",
    "# Simulate true TSI phenotypes using true beta\n",
    "Y_TSI_true = np.dot(beta, X_TSI)\n",
    "\n",
    "# Predict TSI phenotypes using estimated r-values\n",
    "Y_TSI_pred = np.dot(r_values, X_TSI)\n",
    "\n",
    "corr = compute_pearsonr(Y_TSI_true, Y_TSI_pred)\n",
    "chisq = Y_TSI_true.shape[0] * corr**2\n",
    "p_value = chi2.sf(chisq, df=1)\n",
    "print(f\"Prediction correlation (R): {corr}\")\n",
    "print(f\"Prediction R²: {corr**2}\")\n",
    "\n",
    "print(f\"Chi-square: {Y_TSI_true.shape[0] * corr**2}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*comments*\n",
    "\n",
    "The  r2 of the predicted phenotypes (vs. true phenotypes) in the test data is 0.122, and the calculated chi-sq statistic is around 10.74. Therefore, the prediction r2 significantly different from 0 (p-value = 0.001). According to theoretical derivations, the r2 value should be around  (h_g^2 × h_g^2) / (h_g^2 + M/N). Here, h_g^2 = 1, M = 1000 and N = 112. Hence, the theoretical value of r2 is 0.10072. It aligns closely with our derivation, and the slight difference might be due to randomness or the presence of masked values (missing data) in the genome matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Modified prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the simulated CEU training data and TSI test data from (1), implement a polygenic prediction scheme using a P-value threshold, in which only markers with P-values beneath a threshold (for ATT χ2 association test) are included in the predictions.  (TSI test data genotypes can be normalized using allele frequencies from CEU training data.)  Implement this scheme for various choices of P-value thresholds.  How do the results vary?  Discuss. \n",
    "Hint: below are $\\chi^2$ thresholds $\\chi^2_{THRESH}$ corresponding to various $P$-value thresholds $P_{THRESH}$:\n",
    "\n",
    "| $P_{THRESH}$ | 1.0 | 0.5 | 0.2 | 0.1 | 0.05 | 0.02 | 0.01 |\n",
    "|---|---|---|---|---|---|---|---|\n",
    "| $\\chi^2_{THRESH}$ | 0.000 | 0.455 | 1.642 | 2.706 | 3.852 | 5.412 | 6.635 |\n",
    "\n",
    "(You can also just use scipy.stats.chi2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armitage trend test\n",
    "def armitage_trend_test(genotype, phenotype):\n",
    "    if np.ma.isMaskedArray(genotype):\n",
    "        valid = ~genotype.mask\n",
    "    else:\n",
    "        valid = np.ones_like(genotype, dtype=bool)\n",
    "\n",
    "    valid_genotype = genotype[valid]\n",
    "    valid_phenotype = phenotype[valid] \n",
    "\n",
    "    if len(valid_genotype) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    r = np.corrcoef(valid_genotype, valid_phenotype)[0, 1]\n",
    "    if np.isnan(r):\n",
    "        return np.nan\n",
    "\n",
    "    N = len(valid_genotype)\n",
    "    return N * r**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "χ² Threshold\tR²\t\tNum SNPs\tCausal SNPs\n",
      "0.000\t\t0.1221\t\t1000\t\t100\n",
      "0.455\t\t0.1525\t\t507\t\t72\n",
      "1.642\t\t0.1054\t\t229\t\t41\n",
      "2.706\t\t0.0992\t\t127\t\t26\n",
      "3.842\t\t0.0583\t\t65\t\t13\n",
      "5.412\t\t0.0828\t\t25\t\t6\n",
      "6.635\t\t0.0365\t\t12\t\t5\n"
     ]
    }
   ],
   "source": [
    "att_chisq = np.zeros(1000)\n",
    "for i in range(1000):\n",
    "    att_chisq[i] = armitage_trend_test(CEU_geno[i], Y_CEU)\n",
    "\n",
    "# Set threshold\n",
    "chi2_thresholds = [0.000, 0.455, 1.642, 2.706, 3.842, 5.412, 6.635]\n",
    "results = []\n",
    "N = len(Y_TSI_true)\n",
    "\n",
    "for chi2_thresh in chi2_thresholds:\n",
    "    selected = att_chisq > chi2_thresh\n",
    "    num_sig = np.sum(selected)\n",
    "    num_causal = np.sum(selected[:100])\n",
    "\n",
    "    if num_sig == 0:\n",
    "        results.append((chi2_thresh, 0, 1.0, 0, 0))  \n",
    "        continue\n",
    "\n",
    "    weights = np.zeros_like(r_values)\n",
    "    weights[selected] = r_values[selected]\n",
    "\n",
    "    Y_TSI_pred_sig = np.dot(weights, X_TSI)\n",
    "    corr_sig = compute_pearsonr(Y_TSI_true, Y_TSI_pred_sig)\n",
    "    r2_sig = corr_sig ** 2\n",
    "    chisq_stat = N * r2_sig\n",
    "\n",
    "    results.append((chi2_thresh, r2_sig, num_sig, num_causal))\n",
    "\n",
    "print(\"χ² Threshold\\tR²\\t\\tNum SNPs\\tCausal SNPs\")\n",
    "for chi2_t, r2, num, causal in results:\n",
    "    print(f\"{chi2_t:.3f}\\t\\t{r2:.4f}\\t\\t{num}\\t\\t{causal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*comments*\n",
    "\n",
    "Based on the results, we can see that when using all SNPs (χ² > 0.0), R² is 0.1221. However, this includes many non-causal SNPs (900 out of 1000), which likely dilutes the predictive signal. When 507 SNPs (72 causal) are included, we achieve the highest R² among these threshold values. In other words, this threshold captures useful signal without adding too much noise. As we increase the threshold, we keep fewer SNPs. Although they are more strongly associated, we lose too many causal ones, so R² drops. Here, the optimal threshold appears to be around χ² > 0.455 (p = 0.5), where predictive accuracy is maximized without filtering out too many relevant SNPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) BLUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the simulated CEU training data and TSI test data from (1), compute BLUP predictions for TSI test samples. Does the prediction r2 agree with theoretical derivations?\n",
    "(Note: if necessary, you may assume that the total genetic variance is 0.99 and the environmental variance is 0.01 to avoid problems inverting the phenotypic covariance matrix.)\n",
    "(Note: it will be necessary to invert the genetic relationship matrix in order to compute BLUP predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUP Prediction R²: 0.1705653246696614\n"
     ]
    }
   ],
   "source": [
    "M = X_CEU.shape[0]\n",
    "sigma_g2 = 0.99\n",
    "sigma_e2 = 0.01\n",
    "\n",
    "# GRM\n",
    "A = X_CEU.T @ X_CEU / M\n",
    "A_star = X_TSI.T @ X_CEU / M\n",
    "V = sigma_g2 * A + sigma_e2 * np.eye(A.shape[0])\n",
    "V_inv = np.linalg.inv(V)\n",
    "\n",
    "Y_TSI_blup = sigma_g2 * A_star @ V_inv @ Y_CEU\n",
    "\n",
    "# Evaluate\n",
    "corr_blup = compute_pearsonr(Y_TSI_true, Y_TSI_blup)\n",
    "print(f\"BLUP Prediction R²: {corr_blup**2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*comments*\n",
    "\n",
    "According to theoretical derivations, \n",
    "r2 = (h_g^2 × h_g^2) / [h_g^2 + (1 - r^2) × M/N]. Here, h_g^2 = 1, M = 1000 and N = 112. \n",
    "\n",
    "Hence, the theoretical value of BLUP Prediction R² is 0.112. Our results are close to theoretical derivations, but do not match exactly, which might be due to random variation and the presence of masked values (missing data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)  LDpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Using the simulated CEU training data and TSI test data from (1), implement a polygenic prediction scheme using the LDpred method in the special case of no LD between SNPs, for various values of the proportion of causal SNPs.  Does the correct value of the proportion of causal SNPs produce the highest prediction r2? (b) Repeat the simulation using effect sizes for causal SNPs sampled from a normal distribution with standard deviation 0.1 (variance 0.01).  Does the correct value of the proportion of causal SNPs produce the highest prediction r2?\n",
    "(Hint: you can sample values from the standard normal distribution N(0,1) by rejection sampling: sample uniformly from [−10,10] (a standard normal variable is unlikely to be outside this range) and retain the sampled value with probability proportional to the standard normal density.)\n",
    "\n",
    "\n",
    "*Note the statement from the assignment: \"do not use a built-in normally distributed random number generator in problem (4).\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion causal SNPs (p)\tLDpred R²\n",
      "1                        \t0.12205296696984451\n",
      "0.5                      \t0.12193676712119224\n",
      "0.1                      \t0.07987983532121183\n",
      "0.05                     \t0.049392713787376176\n",
      "0.01                     \t0.005951517293815547\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "h_g2 = 1.0  \n",
    "M = 1000\n",
    "N = X_CEU.shape[1]\n",
    "proportion = [1, 0.5, 0.1, 0.05, 0.01]\n",
    "\n",
    "ldpred_results = []\n",
    "\n",
    "for p in proportion:\n",
    "    v1 = h_g2 / (M * p) + 1 / N\n",
    "    v0 = 1 / N\n",
    "\n",
    "    sqrt_v1 = np.sqrt(v1)\n",
    "    sqrt_v0 = np.sqrt(v0)\n",
    "\n",
    "    exp1 = np.exp(-r_values**2 / (2 * v1))\n",
    "    exp0 = np.exp(-r_values**2 / (2 * v0))\n",
    "\n",
    "    numerator = (p / sqrt_v1) * exp1\n",
    "    denominator = numerator + ((1 - p) / sqrt_v0) * exp0\n",
    "    posterior_prob = numerator / denominator \n",
    "\n",
    "    coeff = h_g2 / (h_g2 + (M * p / N))\n",
    "    ldpred_betas = coeff * posterior_prob * r_values\n",
    "\n",
    "    # Predict in TSI\n",
    "    Y_pred = np.dot(ldpred_betas, X_TSI)\n",
    "    r = compute_pearsonr(Y_TSI_true, Y_pred)\n",
    "    r2 = r ** 2\n",
    "    ldpred_results.append((p, r2))\n",
    "\n",
    "print(\"Proportion causal SNPs (p)\\tLDpred R²\")\n",
    "for p, r2 in ldpred_results:\n",
    "    print(f\"{p:<25}\\t{r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion causal SNPs (p)\tLDpred R² (Normal Effects)\n",
      "1                        \t0.028411494958547404\n",
      "0.5                      \t0.04171619339597841\n",
      "0.1                      \t0.10390632713954469\n",
      "0.05                     \t0.10353257559152029\n",
      "0.01                     \t0.029285667123192027\n"
     ]
    }
   ],
   "source": [
    "# Generate random effect sizes\n",
    "def sample_normal(n, std=0.1):\n",
    "    samples = []\n",
    "    while len(samples) < n:\n",
    "        x = np.random.uniform(-10, 10)\n",
    "        accept_prob = np.exp(-x**2 / 2)  # standard normal PDF (unnormalized)\n",
    "        if np.random.uniform(0, 1) < accept_prob:\n",
    "            samples.append(x * std)\n",
    "    return np.array(samples)\n",
    "\n",
    "M = 1000\n",
    "causal_indices = np.zeros(M, dtype=bool)\n",
    "causal_indices[:100] = True\n",
    "random_effect = np.zeros(M)\n",
    "random_effect[causal_indices] = sample_normal(np.sum(causal_indices))\n",
    "\n",
    "Y_CEU_normal = np.dot(random_effect, X_CEU)\n",
    "Y_TSI_true_normal = np.dot(random_effect, X_TSI)\n",
    "\n",
    "r_values_normal = np.zeros(M)\n",
    "for i in range(M):\n",
    "    r_values_normal[i] = compute_pearsonr(X_CEU[i], Y_CEU_normal)\n",
    "\n",
    "# Rerun LDpred with normal effect sizes\n",
    "ldpred_results_normal = []\n",
    "for p in proportion:\n",
    "    v1 = h_g2 / (M * p) + 1 / N\n",
    "    v0 = 1 / N\n",
    "\n",
    "    sqrt_v1 = np.sqrt(v1)\n",
    "    sqrt_v0 = np.sqrt(v0)\n",
    "\n",
    "    exp1 = np.exp(-r_values_normal**2 / (2 * v1))\n",
    "    exp0 = np.exp(-r_values_normal**2 / (2 * v0))\n",
    "\n",
    "    numerator = (p / sqrt_v1) * exp1\n",
    "    denominator = numerator + ((1 - p) / sqrt_v0) * exp0\n",
    "    posterior_prob = numerator / denominator\n",
    "\n",
    "    coeff = h_g2 / (h_g2 + (M * p / N))\n",
    "    ldpred_betas = coeff * posterior_prob * r_values_normal\n",
    "\n",
    "    Y_pred = np.dot(ldpred_betas, X_TSI)\n",
    "    r = compute_pearsonr(Y_TSI_true_normal, Y_pred)\n",
    "    r2 = r ** 2\n",
    "    ldpred_results_normal.append((p, r2))\n",
    "\n",
    "# Print results\n",
    "print(\"Proportion causal SNPs (p)\\tLDpred R² (Normal Effects)\")\n",
    "for p, r2 in ldpred_results_normal:\n",
    "    print(f\"{p:<25}\\t{r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*comments*\n",
    "\n",
    "From the first results above, we observe that even though the true proportion of causal SNPs is 0.1, the highest r2 is achieved when p = 1 or 0.5. This is because in LDpred, we assume that causal SNPs have effect sizes drawn from a normal distribution. However, in this stimulation, we have exactly 100 causal SNPs with constant effect size = 0.1. Because LDpred assumes normal effect sizes, its posterior formula expects a mixture of small and large effects, but our actual effects are all moderate and equal. So when we set p = 0.1, LDpred expects a few large effects and shrinks others hard. But in truth, all 100 causal SNPs have the same size, so even smaller marginal effects are meaningful, and LDpred underestimates the importance of many of them. Higher p (0.5 or 1 in this case) leads to less shrinkage overall, preserving more of the signal. \n",
    "\n",
    "In the second part, the highest r2 (0.1039) is achieved when p = 0.1,  which matches the true proportion of causal SNPs in our simulation. This is consistent with the fact that LDpred performs best when its prior on the proportion of causal variants aligns with reality. When p > 0.1, LDpred under-regularizes, meaning that it includes too many non-causal SNPs. When p < 0.1, LDpred over-shrinks the effect sizes and loses real signal. In short, we see that when SNP effect sizes are truly normally distributed, LDpred achieves optimal prediction accuracy at the correct causal proportion p = 0.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) MLMi mixed model association $\\chi^2$  statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a set of 1,000 unlinked SNPs in the 112 CEU individuals consisting of 100 unlinked SNPs on each of chromosomes 1-10 defined by the first SNP and every 125th SNP thereafter (SNPs 0, 50, …, 12375, if the first SNP is SNP 0) on each of those chromosomes.  Assign the first SNP on each chromosome as a causal SNP (10 causal SNPs total) and the remaining 990 SNPs as non-causal SNPs.  Simulate quantitative phenotypes for CEU individuals by assuming that each causal SNP has effect size per normalized genotype = sqrt(1/10) = 0.316.  Compute MLMi mixed model association χ2 statistics as described in Week 10 slides (EMMAX method).  For simplicity, you can assume σg2 = 1 and σe2 = 0.  How do average MLMi mixed model association χ2 statistics at all, null, and causal SNPs compare to average ATT χ2 association statistics at the same SNPs?  Does this agree with theoretical derivations?  (Note: it will be necessary to invert the genetic relationship matrix in order to compute MLMi mixed model association χ2 statistics.  Built-in matrix inversion routines will be provided. Also, it is fine to use numpy.linalg.inv() to compute the inverse of a matrix.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_geno_pop_chr_slice(pop, chromosome, end, step):\n",
    "    '''Reads a slice of a geno file into a numpy matrix'''\n",
    "    f = popen(pop + '.geno')      # open the file\n",
    "    (start_chr, stop_chr) = get_chr_range(chromosome)\n",
    "    stop = start_chr+end\n",
    "    s = it.islice(f, start_chr,stop,step) # slice the file\n",
    "    return read_geno(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- MLMi Chi-sq\t----------\n",
      "Average χ² for causal SNPs: 8.037746018354785\n",
      "Average χ² for null SNPs: 0.6983087090873453\n",
      "Average χ² for all SNPs: 0.7717030821800197\n",
      "-------- ATT Chi-sq\t----------\n",
      "Average χ² for causal SNPs: 10.079384931376692\n",
      "Average χ² for null SNPs: 0.9752888281713061\n",
      "Average χ² for all SNPs: 1.0663297892033599\n"
     ]
    }
   ],
   "source": [
    "CEU_genos = [read_geno_pop_chr_slice('CEU', chromosome, 12500 ,125) for chromosome in range(1,11)]\n",
    "CEU_geno_whole  = np.ma.vstack(CEU_genos)\n",
    "CEU_whole_norm = normalize_geno(CEU_geno_whole)\n",
    "\n",
    "# Assign effect sizes\n",
    "causal_index_new = np.arange(0, M, 100)\n",
    "effect_sizes = np.zeros(M)\n",
    "effect_sizes[causal_index_new] = 0.316\n",
    "\n",
    "# Simulate phenotype\n",
    "Y_CEU_new = CEU_whole_norm.T.dot(effect_sizes)\n",
    "\n",
    "# Build GRM\n",
    "M = CEU_whole_norm.shape[0]\n",
    "A_new = CEU_whole_norm.T.dot(CEU_whole_norm) / M \n",
    "\n",
    "V_new = sigma_g2 * A_new + sigma_e2 * np.eye(A_new.shape[0])\n",
    "\n",
    "# Compute MLMi statistics\n",
    "MLMi = (CEU_whole_norm.dot(np.linalg.inv(V_new).dot(Y_CEU_new)))**2/CEU_whole_norm.dot(np.linalg.inv(V_new).dot(CEU_whole_norm.T))\n",
    "MLMi_diag = np.diagonal(MLMi)\n",
    "\n",
    "causal_mask = np.zeros(1000, dtype=bool)\n",
    "causal_mask[causal_index_new] = True\n",
    "\n",
    "MLMi_chi2_causal = np.mean(MLMi_diag[causal_mask])\n",
    "MLMi_chi2_null = np.mean(MLMi_diag[~causal_mask])\n",
    "MLMi_chi2_all = np.mean(MLMi_diag)\n",
    "\n",
    "print('-------- MLMi Chi-sq\t----------')\n",
    "print(\"Average χ² for causal SNPs:\", MLMi_chi2_causal)\n",
    "print(\"Average χ² for null SNPs:\", MLMi_chi2_null)\n",
    "print(\"Average χ² for all SNPs:\", MLMi_chi2_all)\n",
    "\n",
    "# Compute ATT statistics using the normalized genotype matrix\n",
    "att_chi2_stats = np.zeros(1000)\n",
    "for i in range(1000):\n",
    "    att_chi2_stats[i] = armitage_trend_test(CEU_whole_norm[i, :], Y_CEU_new) \n",
    "\n",
    "avg_chi2_causal = np.mean(att_chi2_stats[causal_mask])\n",
    "avg_chi2_null = np.mean(att_chi2_stats[~causal_mask])\n",
    "avg_chi2_all = np.mean(att_chi2_stats)\n",
    "\n",
    "print('-------- ATT Chi-sq\t----------')\n",
    "print(\"Average χ² for causal SNPs:\", avg_chi2_causal)\n",
    "print(\"Average χ² for null SNPs:\", avg_chi2_null)\n",
    "print(\"Average χ² for all SNPs:\", avg_chi2_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*comments*\n",
    "The theoretical value of MLMi chi-square statistics:\n",
    "1. MLMi causal: \n",
    "\n",
    "Since E[chi_sq for all SNPs] = P(causal) × E[chi_sq for causal SNPs] + P(null) × E[chi_sq for null SNPs], we can plug in the numbers. \n",
    "1 = 0.01 × E[chi_sq for causal SNPs] + 0.99 × 0.888. \n",
    "E[chi_sq for causal SNPs] = 12.088\n",
    "\n",
    "2. MLMi null: (1 - r2 × h_g^2) / [h_g^2 × N/ M + (1 - r2 × h_g^2)]. Here, r2 ~ 0.112 as previously derived, so MLMi null ~ 0.888\n",
    "3. MLMi all: 1\n",
    "\n",
    "The theoretical value of ATT chi-square statistics:\n",
    "1. ATT causal: 1 + (h_g^2 × N/ M_causal) = 1 + 1 × 112/10 = 12.2\n",
    "2. ATT null: 1\n",
    "3. ATT all: 1 + (h_g^2 × N/ M) = 1 + 1 × 112/1000 = 1.112\n",
    "\n",
    "Based on the results above, we observe that MLMi tends to deflate average test statistics, making it more conservative. This is especially evident for null SNPs, which are corrected too aggressively, and for causal SNPs, whose chi-sq values fall slightly below the theoretical expectation. This deflation arises because MLMi uses a GRM that includes causal SNPs, absorbing part of their signal and leading to a slight loss of power.\n",
    "\n",
    "In contrast, ATT results show mild inflation, with average χ² values slightly above 1, which is consistent with expectations under a polygenic model. While the average χ² for causal SNPs is slightly below the theoretical value of 12.2, it remains close, indicating that ATT retains more of the causal signal.\n",
    "\n",
    "In conclusion, MLMi offers better control of false positives but can reduce power due to overcorrection, particularly when causal variants are included in the GRM. ATT retains higher power but is more susceptible to inflation from polygenic background effects."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
